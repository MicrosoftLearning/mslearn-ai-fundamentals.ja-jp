---
lab:
  title: Microsoft Foundry で音声の使用を開始する
  description: Microsoft Foundry を使用して、Azure Speech - 音声ライブを試します。
---

# Microsoft Foundry で音声の使用を開始する

音声対応アプリケーションを強化する 2 つの基本的な音声機能は、音声認識 (単語の音声をテキストに変換する) と音声合成 (テキストを自然な音声に変換する) です。 

この演習では、AI アプリケーションを作成するための Microsoft のプラットフォームである Microsoft Foundry を使用して、生成 AI モデルと音声を使用して対話します。 リアルタイム音声ベースのエージェントを構築するために使用されるサービスである Azure Speech 音声ライブを使用した、音声テキスト変換 (STT) とテキスト音声変換 (TTS) について見ていきます。 音声プレイグラウンドの音声ライブでは、音声認識と音声合成の両方がサポートされているため、モデルと音声ベースの会話ができます。

この演習は約 **30** 分かかります。

## Microsoft Foundry プロジェクトを作成する

Microsoft Foundry でプロジェクトを使用して、AI ソリューションの開発に使用されるモデル、リソース、データ、その他の資産を整理します。

1. Web ブラウザーで、https://ai.azure.com から Microsoft Foundry を開き、Azure 資格情報を使用してサインインします。 初めてサインインするときに開いたヒントやクイック スタート ペインを閉じます。また、必要に応じて、左上にある Foundry ロゴを使用してホーム ページに移動します。

1. まだ有効になっていない場合は、ページ上部のツール バーで [新しい Foundry] オプションを有効にします。 次に、メッセージが表示されたら、一意の名前を持つ新しいプロジェクトを作成します。 **[詳細オプション]** を展開し、プロジェクト用に次の設定を指定します。
    - **Foundry リソース**: *AI Foundry リソースに有効な名前を入力します。*
    - **[サブスクリプション]**:"*ご自身の Azure サブスクリプション*"
    - **リソース グループ**: *リソース グループを作成または選択します*
    - **[リージョン]**: *[米国東部 2]*、*[米国西部 2]*、*[スウェーデン中部]*、*[西ヨーロッパ]*、*[オーストラリア東部]*、*[東南アジア]*、*[東日本]*、または *[インド中部]* のいずれかのリージョンを選択します\*
    
    \**モデル デプロイは、リージョンのクォータによって制限されます。使用可能なクォータが不足しているリージョンを選択した場合、後で新しいリソースの代替リージョンを選択しなければならない場合があります。*

1. **［作成］** を選択します プロジェクトが作成されるまで待ちます。 新しい Foundry ポータルでプロジェクトを作成または選択すると、それが次の画像のようなページで開かれます。

    ![新しい Foundry ホーム ページのスクリーンショット](./media/0126-new-foundry-project.png)

## Azure Speech - 音声ライブに移動する 

1. 新しい Foundry ホーム ページで、右上のメニューに移動します (メニュー オプションを表示するには、画面を展開することが必要な場合があります)。 **[Build]\(ビルド\)** を選択します。 

    ![[ビルド] メニュー オプションに移動する方法のスクリーンショット。](./media/0126-new-foundry-home-build-selected.png)
  
1. *[ビルド]* ページで、左側のメニューに移動します (オプションを表示するには、展開が必要になる場合があります)。 メニューから **[モデル]** を選択します。 [モデル] ページで、**[AI-services]** を選択します。 AI サービスの一覧が、Foundry Tools で使用できるすべての AI 機能の小さなサブセットになっていることを確認してください。 次のような、いくつかの音声関連のサービスをテストに使用できます。 
    - *Azure Speech - 音声テキスト変換*: 音声オーディオからテキストの文字起こしを生成するために使用される機能。 たとえば、通話や会議の文字起こしをしたり、耳が不自由なユーザー向けにキャプションを作成したりします。
    - *Azure Speech - テキスト音声変換*: テキストからオーディオを生成するために使用される機能。 たとえば、目が不自由なユーザーを支援するオーディオを作成したり、音声が自然に聞こえるボットを有効にしたりします。

1. 一覧から **[Azure Speech - 音声ライブ]** を選択して、音声プレイグラウンドで *[音声ライブ]* 機能を試します。 

    ![音声プレイグラウンドに移動して [Azure Speech - 音声ライブ] をテストする方法のスクリーンショット。](./media/0126-new-foundry-ai-services-voice-live.png)

## 音声プレイグラウンド アプリを開く

まずは、生成 AI モデルとチャットしてみましょう。 この演習では、ブラウザー ベースのアプリケーションを使用して、**GPT-4.1 Mini** モデルとチャットします。これは、一般的なチャット ソリューションに役立つ小さな言語モデルです。

1. Web ブラウザーで、Azure Speech - 音声ライブの音声プレイグラウンドを開く必要があります。 プレイグラウンドの設定ウィンドウでサンプルをクリックし、**[空白から開始]** を選択して、独自のアシスタントを作成します。 
 
1. プレイグラウンドの設定ウィンドウで、アシスタントで使用される **[生成 AI モデル]** を変更します。 **[GPT-4.1 Mini]** を選択します。 **[変更の適用]** を選択して、更新内容を保存します。 

1. 次のような音声プレイグラウンド アプリが表示されます。

    ![GPT-4.1 Mini と空白のチャットが選択されている音声ライブのスクリーンショット。](./media/0126-new-foundry-blank-start.png) 

## 音声を選択する 

テキスト音声変換ソリューションでは、音声を使用して、生成された音声のリズム、発音、音質、その他の側面を制御します。 使用可能な音声は、ブラウザーとオペレーティング システムによって異なります。

1. 左側の構成ウィンドウで、**[音声出力]** ドロップダウン リストに音声が表示されます。
 
1. 使用可能な音声のいずれかを選択し、選択した音声の [プレビュー] (▷) ボタンを使用して音声のサンプルを聞きます。
 
1. 使用する音声を選択したら、**[変更の適用]** ボタンを使用して、変更をアクティブにします。

## 音声を使用してモデルと対話する

このアプリでは音声認識と音声合成の両方がサポートされているため、モデルと音声ベースで会話することができます。

1. [チャット] ペインで **[開始]** ボタンを使用して、モデルとの会話を開始します。 メッセージが表示されたら、システム マイクへのアクセスを許可します。 エージェントが自己紹介します。 

    >**ヒント**: マイクへのアクセスを求められず、マイクが検出されない場合は、次の手順を試してマイクへのアクセスを許可してください。 ブラウザー ウィンドウで、ページの URL に移動します。 URL の横にある*ロック アイコン*をクリックします。 *[アクセス許可]*、*[マイク]*、および *[許可]* を選択します。 その後、ページを更新して、もう一度お試しください。

1. アプリの状態が **Listening…** の場合は、`"How does speech recognition work?"` などと話しかけ、応答を待ちます。

1. アプリの状態が **Processing…** に変わったことを確認します。 アプリが音声入力を処理し、音声テキスト変換を使用して音声をテキストに変換し、プロンプトとしてモデルに送信します。 

    >**ヒント**: 処理速度が非常に速く、実際には、状態が *[話し中]* に戻る前に、状態が表示されない場合があります。

1. 状態が **Speaking...** に変わると、アプリはテキスト音声変換を使用してモデルからの応答を音声化します。 元のプロンプトと応答をテキストとして表示するには、チャット画面の下部にある **[cc]** ボタンを選択します。

    >**ヒント**: 後続のプロンプトは、話すだけで送信されます。 エージェントを中断して、必要な作業に集中して操作を続けることもできます。 
    >**ヒント**: チャット ペインの [生成停止] ボタンを使用して、実行時間の長い応答を停止することもできます。 このボタンで会話が終了します。 エージェントを引き続き使用するには、新しい会話を開始する必要があります。 

    ![クローズド キャプションを表示する [CC] ボタンが選択されているスクリーンショット。](./media/0126-new-foundry-voice-show-text.png)

1. 会話を続けるには、`"How does speech synthesis work?"` などの 2 つ目の音声プロンプトを送信し、応答を確認します。

## システム プロンプトを試す

システム プロンプトは、応答をガイドする指示をモデルに提供するために使用します。 システム プロンプトを使用して、モデルの応答が何を含む必要があり、何を含んではならないかについて、形式、スタイル、制約に関するガイドラインを提供できます。

1. 左側のペインの **[指示]** テキスト領域で、システム プロンプトを次のように変更します: `You are an AI assistant that provides short and concise answers using simple language. Limit responses to a single sentence.`

    >**ヒント**: 変更をテストする前に、**[変更の適用]** を選択します。 

1. 次に、前と同じプロンプト (`How does speech synthesis work?`) を試し、出力を確認します。

## モデル パラメーターを試す

モデル パラメーターはモデルがどのように動作するかを制御するもので、応答のサイズ (トークン単位で測定される) を制限し、応答の "創造性" の程度を制御するのに役立ちます。

#### 生成 AI モデルのパラメーター

1. 生成 AI モデルの *[詳細設定]* を確認します。 モデルの応答に影響を与える方法の 1 つが、応答の**温度**を構成することです。 [温度] は、モデルの応答のランダム性または創造性を制御するパラメーターです。** モデルが低い温度に設定されると、その応答はより予測可能で事実に基づいたものになります。 温度が上昇すると、変動性と創造性が向上します。 温度を高く設定することは、ブレーンストーミングや、会話のトーン、さまざまな例の生成で便利です。 ただし、温度が高すぎると、応答が意味をなさず、信頼性が低下する可能性があります。

1. 温度を変更し、前と同じプロンプト (`How does speech synthesis work?`) を繰り返して実験しましょう。 

1. モデルのもう 1 つの設定が、**事前エンゲージメント**です。 **オン**の切り替えをアクティブにすると、エージェントが会話を開始します。 事前エンゲージメントを*オン*にして、エージェントとの新しい会話を開始してみてください。  

#### 音声入力パラメーター 

1. 音声入力の *[詳細設定]* を確認します。 
- **[発話の終了 (EOU)]**:発話の終了を検出し、音声認識処理を停止し、結果をすぐに返します。 現在、GPT-4o Realtime または GPT-4o Mini Realtime モデルはサポートされていません。
- **[オーディオの改良]**:ノイズを減らし、明瞭さを高め、より正確で明瞭な音声認識を実現することで、音質を向上させます。

#### 音声出力パラメーター

1. 音声出力の詳細設定を確認します。 
- **[音声温度]**:イントネーション、韻律、強弱、ペース、感情の変化など、音声オーディオのスタイルと表現力を制御します。 
- **[再生速度]**:音声が話されている速度。
- **[カスタム辞書]**:会社名、医療用語、絵文字など、特定の単語の発音を定義します。 Audio Content Creation ツールを使用してカスタム辞書ファイルを作成し、ここにそのリンクをコピーして使用します

1. 時間がある場合は、Azure アバターを試すこともできます。 アバターの切り替えをアクティブにすると、事前構築済みのアバターを選択したり、エージェントのオーディオ出力をアバターが話しているように視覚化するカスタム アバターを作成したりできます。

## クライアント コードを表示する 

次に、この Web エクスペリエンスを可能にするコードを確認することにしましょう。

1. チャット画面の上部にある **[コード]** を選択します。 次のような Python コードが表示されます。  

    ![音声ライブ アプリの Python コードの開始を示すスクリーンショット。](./media/0126-voice-live-code-start.png)

1. 行 `17-32` で、インポートされた特定の Azure Speech パッケージを確認できます。 インポートされたパッケージによって、追加の機能とツールが提供されます。この場合は、会話テキスト自体に応答するために使用される言語モデルを補完する追加の関数とモデルです。 これらのパッケージをインポートすると、すべてを最初から記述する代わりに、事前構築済みの最適化されたソリューションを活用できるため、コードの効率性、読みやすさ、保守性が向上します。  

    ![インポートされたパッケージのスクリーンショット。](./media/0126-voice-live-azure-imports.png)
 
1. Web ライブ音声アシスタントは、オーディオ プロセッサと音声アシスタントの 2 つの主要な機能で構成されています。  行 `63-238` で、`AudioProcessor` クラスのコードを確認すると、リアルタイムのオーディオ キャプチャと再生がどのように処理されるかがわかります。 

    ![オーディオ プロセッサ クラスのスクリーンショット。](./media/0126-voice-live-audio-processor.png)

1. `BasicVoiceAssistant` クラスは、行 `240` から始まります。 このクラスのコードでは、VoiceLive Python SDK を使用して VoiceLive 接続からのイベントを処理します。 `BasicVoiceAssistant` が `AudioProcessor` クラス (行`258`など) にどのように依存しているかに注意してください。   

    ![音声アシスタント クラスのスクリーンショット。](./media/0126-voice-live-basic-voice-assistant.png)

1. プレイグラウンド設定と資格情報の構成 (AI の音声、モデル、命令など) は、行 `417` から始まる `parse_arguments` グローバル関数によって処理されます。

    ![parse arguments 関数のスクリーンショット。](./media/0126-voice-live-parse-arguments.png)

1. VoiceLive 資格情報を？するには、コード画面の上部にある **{X}.env 変数** をクリックします。

    ![env 変数のスクリーンショット。](./media/0126-voice-live-env.png)

1. すべてをまとめると、行 `472` から始まる `main` 関数で実行される内容を理解できます。 
    - Azure 資格情報が検証されます ("parse_arguments() が変数 `args` にどのように保存されるかに注意してください")**
    - クライアントが作成されます
    - 音声アシスタントが作成されます ("行 `497` で `BasicVoiceAssistant` を呼び出してアシスタントが作成される方法に注意してください")**
    - 音声アシスタントには、適切なシャットダウンのためのコードが与えられます
    - 音声アシスタントが起動されます 

    ![main 関数のスクリーンショット](./media/0126-voice-live-main-function.png)

>**お試しください**:職場または学校アカウントを持っているユーザーは、コード ウィンドウの上部にある **[Web 用の VS Code で開く]** をクリックして、手順に従うことができます。  
    
## クリーンアップ

Microsoft Foundry の調査が完了したら、不要になったリソースをすべて削除します。 これにより、不要なコストが発生することを防ぎます。

1. [https://portal.azure.com](https://portal.azure.com) で **Azure portal** を開き、作成したリソースを含むリソース グループを選択します。
1. **[リソース グループの削除]** を選び、**リソース グループの名前を入力**して、確定します。 これでリソース グループが削除されます。