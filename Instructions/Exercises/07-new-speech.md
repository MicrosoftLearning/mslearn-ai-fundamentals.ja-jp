---
lab:
  title: 新しい Microsoft Foundry ポータルの音声について確認する
---

# 新しい Microsoft Foundry ポータルの音声について確認する

この演習では、Microsoft のプラットフォームを使用して AI アプリケーションである Microsoft Foundry を作成し、音声を使用して生成 AI モデルと対話します。 エージェント アプリケーションを通じて、Azure Speech の音声テキスト変換 (STT) とテキスト音声変換 (TTS) の機能を確認します。

> **注**:この演習は、新しい Foundry ポータルを使用してモデルのトレーニングとテストを行う手順を説明することを目的としています。 十分なアクセス許可を持つ Azure サブスクリプションがある場合は、Foundry プロジェクトを演習用にプロビジョニングして使用することができます。 ただし、Foundry は、膨大な量のデータとクラウドベースのコンピューティングを含むエンタープライズ規模の機械学習ソリューション向けに設計されています。 Azure にアクセスできない場合、または限られた時間で演習を完了する必要がある場合は、この演習で使用する Azure Speech のコア機能を含むブラウザーベースの Lab アプリが `https://aka.ms/speech-playground` で提供されています。 Web ベースの Lab のユーザー インターフェイスは Foundry と同じではありませんが、Foundry への移行を直感的に理解するには十分に類似しています。 Web ベースの Lab アプリはブラウザーで実行されるため、いつでもページを更新するとアプリが再起動されることに注意してください。

この演習は約 **20** 分かかります。

## Microsoft Foundry でプロジェクトを作成する

1. Web ブラウザーで、[Microsoft Foundry](https://ai.azure.com) (`https://ai.azure.com`) を開き、Azure 資格情報を使用してサインインします。 初めてサインインする場合に開かれるヒントまたはクイック スタートのペインを閉じ、必要に応じて、左上にある **[Foundry]** ロゴを使用してホーム ページに移動します。次の図のようなページが表示されます (**[ヘルプ]** ペインが表示される場合は閉じます)。

    ![新しい Foundry のトグルが強調表示されている Microsoft Foundry ホーム ページのスクリーンショット。](./media/foundry-home-page-classic.png)

1. 画面の上部にある **[新しい Foundry]** のトグルを選択します。 

1. 新しい Foundry ユーザー インターフェイスを利用するには、サポートされているリージョンにプロジェクトを作成する必要があります。 ドロップダウン メニューで **[新しいプロジェクトの作成]** を選択します。 (*注*: サブスクリプションに他のプロジェクトが作成されていて、サポートされているリージョンにデプロイされている場合は、ドロップダウン リストにも表示されます)。

    ![新しい Foundry UI にアクセスするためのプロジェクト選択メニューのスクリーンショット。](./media/create-project-new-foundry.png)

1. **プロジェクトの作成**ウィザードで、プロジェクトの有効な名前を入力します。 次に、**[詳細オプション]** を展開し、プロジェクト用に次の設定を指定します。
    - **Foundry リソース**: *AI Foundry リソースに有効な名前を入力します。*
    - **[サブスクリプション]**:"*ご自身の Azure サブスクリプション*"
    - **リソース グループ**: *リソース グループを作成または選択します*
    - **[リージョン]**: **Foundry で推奨される**リージョンのいずれかを選択します\*
    
    \**モデル デプロイは、リージョンのクォータによって制限されます。使用可能なクォータが不足しているリージョンを選択した場合、後で新しいリソースの代替リージョンを選択しなければならない場合があります。*

1. **［作成］** を選択します プロジェクトが作成されるまで待ちます。 これには数分かかることがあります。

## Azure Speech - 音声ライブに移動する 

1. プロジェクトが作成されると、新しい Foundry ホーム ページが表示されます。 ホーム ページで、右上のメニューに移動します (メニュー オプションを表示するには、画面を展開する必要がある場合があります)。 **[Build]\(ビルド\)** を選択します。 

    ![[ビルド] メニュー オプションに移動する方法のスクリーンショット。](./media/0126-new-foundry-home-build-selected.png)
  
1. *[ビルド]* ページで **[モデル]** を選択し、**[AI サービス]** を選択します。 AI サービスの一覧が、Foundry Tools で使用できるすべての AI 機能の小さなサブセットになっていることを確認してください。 一覧から **[Azure Speech - 音声ライブ]** を選択して、音声プレイグラウンドで *[音声ライブ]* 機能を試します。 

    ![音声プレイグラウンドに移動して [Azure Speech - 音声ライブ] をテストする方法のスクリーンショット。](./media/0126-new-foundry-ai-services-voice-live.png)

音声対応アプリケーションを強化する 2 つの基本的な音声機能は、音声認識 (単語の音声をテキストに変換する) と音声合成 (テキストを自然な音声に変換する) です。 音声プレイグラウンドの音声ライブでは、音声認識と音声合成の両方がサポートされているため、モデルと音声ベースの会話ができます。 音声ライブは、Azure Speech のいくつかの機能を組み合わせたものです。 

## 音声プレイグラウンド アプリを開く

まずは、生成 AI モデルとチャットしてみましょう。 この演習では、ブラウザーベースのアプリケーションを使用して、**Microsoft Phi 4 Mini** モデル (低帯域幅シナリオの一般的なチャット ソリューションに役立つ小さな言語モデル) とチャットします。 

1. Web ブラウザーで、Azure Speech - 音声ライブの音声プレイグラウンドを開く必要があります。 プレイグラウンドの設定ウィンドウでサンプルをクリックし、**[空白から開始]** を選択して、独自のアシスタントを作成します。 
 
1. プレイグラウンドの設定ウィンドウで、アシスタントで使用される **[生成 AI モデル]** を変更します。 *Microsoft Phi 4 mini* モデルを使用するには、**[Phi4 Mini (プレビュー)]** を選択します。 **[変更の適用]** を選択して、更新内容を保存します。 

1. 次のような音声プレイグラウンド アプリが表示されます。

    ![Phi4 Mini と空白のチャットが選択されている音声ライブのスクリーンショット。](./media/0126-new-foundry-blank-start.png) 

## 音声を選択する 

テキスト音声変換ソリューションでは、音声を使用して、生成された音声のリズム、発音、音質、その他の側面を制御します。 使用可能な音声は、ブラウザーとオペレーティング システムによって異なります。

1. 左側の構成ウィンドウで、**[音声出力]** ドロップダウン リストに音声が表示されます。
 
1. 使用可能な音声のいずれかを選択し、選択した音声の [プレビュー] (▷) ボタンを使用して音声のサンプルを聞きます。
 
1. 使用する音声を選択したら、**[変更の適用]** ボタンを使用して、変更をアクティブにします。

## 音声を使用してモデルと対話する

このアプリでは音声認識と音声合成の両方がサポートされているため、モデルと音声ベースで会話することができます。

1. [チャット] ペインで **[開始]** ボタンを使用して、モデルとの会話を開始します。 メッセージが表示されたら、システム マイクへのアクセスを許可します。 エージェントが自己紹介します。 

1. アプリの状態が **Listening…** の場合は、`"How does speech recognition work?"` などと話しかけ、応答を待ちます。

    >**ヒント**: エラーが発生した場合、またはアプリが音声入力を検出できない場合は、テキストベースのプロンプトを入力できます。 

1. アプリの状態が **Processing…** に変わったことを確認します。 アプリが音声入力を処理し、音声テキスト変換を使用して音声をテキストに変換し、プロンプトとしてモデルに送信します。 

    >**ヒント**: 処理速度が非常に速いので、実際には、状態が *Speaking* に戻る前に、Processing… が表示されない場合があります。

1. 状態が **Speaking...** に変わると、アプリはテキスト音声変換を使用してモデルからの応答を音声化します。 元のプロンプトと応答をテキストとして表示するには、チャット画面の下部にある **[cc]** ボタンを選択します。

    ![クローズド キャプションを表示する [CC] ボタンが選択されているスクリーンショット。](./media/0126-new-foundry-voice-show-text.png)

Azure Speech 音声ライブは、リアルタイムの音声ベースのエージェントを構築するために使用されるサービスです。 Microsoft Foundry の Azure Speech 音声ライブ機能には、中断とバックグラウンド ノイズ抑制をサポートするリアルタイムのマルチターン会話が含まれています。

1. 会話を続けるには、`"How does speech synthesis work?"` などの 2 つ目の音声プロンプトを送信し、応答を確認します。

    >**ヒント**: 生成 AI モデルの *[詳細設定]* を確認します。 モデルの応答に影響を与えるもう 1 つの方法は、応答の *[温度]* を構成することです。 *[温度]* は、モデルの応答のランダム性または創造性を制御するパラメーターです。 モデルが低い温度に設定されると、その応答はより予測可能で事実に基づいたものになります。 温度が上昇すると、変動性と創造性が向上します。 温度を高く設定することは、ブレーンストーミングや、会話のトーン、さまざまな例の生成で便利です。 ただし、温度が高すぎると、応答が意味をなさず、信頼性が低下する可能性があります。

## コードの確認 

次に、この Web エクスペリエンスを可能にするコードを確認することにしましょう。

1. チャット画面の上部にある **[コード]** を選択します。 次のような Python コードが表示されます。  

    ![音声ライブ アプリの Python コードの開始を示すスクリーンショット。](./media/0126-voice-live-code-start.png)

1. 行 `17-32` で、インポートされた特定の Azure Speech パッケージを確認できます。 インポートされたパッケージによって、追加の機能とツールが提供されます。この場合は、会話テキスト自体に応答するために使用される言語モデルを補完する追加の関数とモデルです。 これらのパッケージをインポートすると、すべてを最初から記述する代わりに、事前構築済みの最適化されたソリューションを活用できるため、コードの効率性、読みやすさ、保守性が向上します。  

    ![インポートされたパッケージのスクリーンショット。](./media/0126-voice-live-azure-imports.png)
 
1. Web ライブ音声アシスタントは、オーディオ プロセッサと音声アシスタントの 2 つの主要な機能で構成されています。  行 `63-238` で、`AudioProcessor` クラスのコードを確認すると、リアルタイムのオーディオ キャプチャと再生がどのように処理されるかがわかります。 

    ![オーディオ プロセッサ クラスのスクリーンショット。](./media/0126-voice-live-audio-processor.png)

1. `BasicVoiceAssistant` クラスは、行 `240` から始まります。 このクラスのコードでは、VoiceLive Python SDK を使用して VoiceLive 接続からのイベントを処理します。 `BasicVoiceAssistant` が `AudioProcessor` クラス (行`258`など) にどのように依存しているかに注意してください。   

    ![音声アシスタント クラスのスクリーンショット。](./media/0126-voice-live-basic-voice-assistant.png)

1. プレイグラウンド設定と資格情報の構成 (AI の音声、モデル、命令など) は、行 `417` から始まる `parse_arguments` グローバル関数によって処理されます。

    ![parse arguments 関数のスクリーンショット。](./media/0126-voice-live-parse-arguments.png)

1. VoiceLive 資格情報を？するには、コード画面の上部にある **{X}.env 変数** をクリックします。

    ![env 変数のスクリーンショット。](./media/0126-voice-live-env.png)

1. すべてをまとめると、行 `472` から始まる `main` 関数で実行される内容を理解できます。 
    - Azure 資格情報が検証されます ("parse_arguments() が変数 `args` にどのように保存されるかに注意してください")**
    - クライアントが作成されます
    - 音声アシスタントが作成されます ("行 `497` で `BasicVoiceAssistant` を呼び出してアシスタントが作成される方法に注意してください")**
    - 音声アシスタントには、適切なシャットダウンのためのコードが与えられます
    - 音声アシスタントが起動されます 

    ![main 関数のスクリーンショット](./media/0126-voice-live-main-function.png)

これでコードの確認が終わりました。次の手順では、コードを自分で試して実装します。 必要なアクセス権がある場合は、VS Code でコードを自分で実行してみることをお勧めします。 

## クリーンアップ

これ以上の演習を行わない場合は、不要になったリソースを削除します。 これにより、不要なコストが発生することを防ぎます。

1. [https://portal.azure.com](https://portal.azure.com) で **Azure portal** を開き、作成したリソースを含むリソース グループを選択します。
1. **[リソース グループの削除]** を選び、**リソース グループの名前を入力**して、確定します。 これでリソース グループが削除されます。