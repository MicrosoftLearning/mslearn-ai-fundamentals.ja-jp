---
lab:
  title: 新しい Microsoft Foundry ポータルの音声について確認する
---

# 新しい Microsoft Foundry ポータルの音声について確認する

音声対応アプリケーションを強化する 2 つの基本的な音声機能は、音声認識 (単語の音声をテキストに変換する) と音声合成 (テキストを自然な音声に変換する) です。 音声プレイグラウンドの音声ライブでは、音声認識と音声合成の両方がサポートされているため、モデルと音声ベースの会話ができます。 Azure Speech 音声ライブは、リアルタイムの音声ベースのエージェントを構築するために使用されるサービスです。 音声ライブは、Azure Speech のいくつかの機能を組み合わせたものです。 

この演習では、Microsoft のプラットフォームを使用して AI アプリケーションである Microsoft Foundry を作成し、音声を使用して生成 AI モデルと対話します。 音声ライブ エージェント アプリケーションを使用して、Azure Speech の音声テキスト変換 (STT) とテキスト読み上げ (TTS) 機能を確認します。

この演習は約 **30** 分かかります。

## Microsoft Foundry プロジェクトを作成する

Microsoft Foundry でプロジェクトを使用して、AI ソリューションの開発に使用されるモデル、リソース、データ、その他の資産を整理します。

1. Web ブラウザーで、https://ai.azure.com から Microsoft Foundry を開き、Azure 資格情報を使用してサインインします。 初めてサインインするときに開いたヒントやクイック スタート ペインを閉じます。また、必要に応じて、左上にある Foundry ロゴを使用してホーム ページに移動します。

1. まだ有効になっていない場合は、ページ上部のツール バーで [新しい Foundry] オプションを有効にします。 次に、メッセージが表示されたら、既定のオプションを使用して、任意の一意の名前で新しいプロジェクトを作成します。 新しい Foundry ポータルでプロジェクトを作成または選択すると、それが次の画像のようなページで開かれます。

    ![新しい Foundry ホーム ページのスクリーンショット](./media/0126-new-foundry-project.png)

## Azure Speech - 音声ライブに移動する 

1. 新しい Foundry ホーム ページで、右上のメニューに移動します (メニュー オプションを表示するには、画面を展開することが必要な場合があります)。 **[Build]\(ビルド\)** を選択します。 

    ![[ビルド] メニュー オプションに移動する方法のスクリーンショット。](./media/0126-new-foundry-home-build-selected.png)
  
1. *[ビルド]* ページで **[モデル]** を選択し、**[AI サービス]** を選択します。 AI サービスの一覧が、Foundry Tools で使用できるすべての AI 機能の小さなサブセットになっていることを確認してください。 一覧から **[Azure Speech - 音声ライブ]** を選択して、音声プレイグラウンドで *[音声ライブ]* 機能を試します。 

    ![音声プレイグラウンドに移動して [Azure Speech - 音声ライブ] をテストする方法のスクリーンショット。](./media/0126-new-foundry-ai-services-voice-live.png)

## 音声プレイグラウンド アプリを開く

まずは、生成 AI モデルとチャットしてみましょう。 この演習では、ブラウザー ベースのアプリケーションを使用して、**GPT-4.1 Mini** モデルとチャットします。これは、一般的なチャット ソリューションに役立つ小さな言語モデルです。

1. Web ブラウザーで、Azure Speech - 音声ライブの音声プレイグラウンドを開く必要があります。 プレイグラウンドの設定ウィンドウでサンプルをクリックし、**[空白から開始]** を選択して、独自のアシスタントを作成します。 
 
1. プレイグラウンドの設定ウィンドウで、アシスタントで使用される **[生成 AI モデル]** を変更します。 **[GPT-4.1 Mini]** を選択します。 **[変更の適用]** を選択して、更新内容を保存します。 

1. 次のような音声プレイグラウンド アプリが表示されます。

    ![GPT-4.1 Mini と空白のチャットが選択されている音声ライブのスクリーンショット。](./media/0126-new-foundry-blank-start.png) 

## 音声を選択する 

テキスト音声変換ソリューションでは、音声を使用して、生成された音声のリズム、発音、音質、その他の側面を制御します。 使用可能な音声は、ブラウザーとオペレーティング システムによって異なります。

1. 左側の構成ウィンドウで、**[音声出力]** ドロップダウン リストに音声が表示されます。
 
1. 使用可能な音声のいずれかを選択し、選択した音声の [プレビュー] (▷) ボタンを使用して音声のサンプルを聞きます。
 
1. 使用する音声を選択したら、**[変更の適用]** ボタンを使用して、変更をアクティブにします。

## 音声を使用してモデルと対話する

このアプリでは音声認識と音声合成の両方がサポートされているため、モデルと音声ベースで会話することができます。

1. [チャット] ペインで **[開始]** ボタンを使用して、モデルとの会話を開始します。 メッセージが表示されたら、システム マイクへのアクセスを許可します。 エージェントが自己紹介します。 

1. アプリの状態が **Listening…** の場合は、`"How does speech recognition work?"` などと話しかけ、応答を待ちます。

    >**ヒント**: エラーが発生した場合、またはアプリが音声入力を検出できない場合は、テキストベースのプロンプトを入力できます。 

1. アプリの状態が **Processing…** に変わったことを確認します。 アプリが音声入力を処理し、音声テキスト変換を使用して音声をテキストに変換し、プロンプトとしてモデルに送信します。 

    >**ヒント**: 処理速度が非常に速いので、実際には、状態が *Speaking* に戻る前に、Processing… が表示されない場合があります。

1. 状態が **Speaking...** に変わると、アプリはテキスト音声変換を使用してモデルからの応答を音声化します。 元のプロンプトと応答をテキストとして表示するには、チャット画面の下部にある **[cc]** ボタンを選択します。

    >**ヒント**: 後続のプロンプトは、話すだけで送信されます。 エージェントを中断して、必要な作業に集中して操作を続けることもできます。 

    ![クローズド キャプションを表示する [CC] ボタンが選択されているスクリーンショット。](./media/0126-new-foundry-voice-show-text.png)

1. 会話を続けるには、`"How does speech synthesis work?"` などの 2 つ目の音声プロンプトを送信し、応答を確認します。

## システム プロンプトを試す

システム プロンプトは、応答をガイドする指示をモデルに提供するために使用します。 システム プロンプトを使用して、モデルの応答が何を含む必要があり、何を含んではならないかについて、形式、スタイル、制約に関するガイドラインを提供できます。

1. 左側のペインの **[指示]** テキスト領域で、システム プロンプトを次のように変更します: `You are an AI assistant that provides short and concise answers using simple language. Limit responses to a single sentence.`

    >**ヒント**: 変更をテストする前に、**[変更の適用]** を選択します。 

1. 次に、前と同じプロンプト (`How does speech synthesis work?`) を試し、出力を確認します。

1. さまざまなシステム プロンプトを続けて試し、モデルによって返される応答の種類に影響を与えてみましょう。

## モデル パラメーターを試す

モデル パラメーターはモデルがどのように動作するかを制御するもので、応答のサイズ (トークン単位で測定される) を制限し、応答の "創造性" の程度を制御するのに役立ちます。

1. 生成 AI モデルの *[詳細設定]* を確認します。 モデルの応答に影響を与える方法の 1 つは、応答の [温度] を構成することです。** [温度] は、モデルの応答のランダム性または創造性を制御するパラメーターです。** モデルが低い温度に設定されると、その応答はより予測可能で事実に基づいたものになります。 温度が上昇すると、変動性と創造性が向上します。 温度を高く設定することは、ブレーンストーミングや、会話のトーン、さまざまな例の生成で便利です。 ただし、温度が高すぎると、応答が意味をなさず、信頼性が低下する可能性があります。

1. パラメーター値を変更し、同じプロンプトを繰り返して実験します。 モデルと動作に違いがある場合があります。 

    >**ヒント**: 実行時間の長い応答は、チャット ペインの [生成の停止] ボタンを使用して停止できます。

## クライアント コードを表示する 

次に、この Web エクスペリエンスを可能にするコードを確認することにしましょう。

1. チャット画面の上部にある **[コード]** を選択します。 次のような Python コードが表示されます。  

    ![音声ライブ アプリの Python コードの開始を示すスクリーンショット。](./media/0126-voice-live-code-start.png)

1. 行 `17-32` で、インポートされた特定の Azure Speech パッケージを確認できます。 インポートされたパッケージによって、追加の機能とツールが提供されます。この場合は、会話テキスト自体に応答するために使用される言語モデルを補完する追加の関数とモデルです。 これらのパッケージをインポートすると、すべてを最初から記述する代わりに、事前構築済みの最適化されたソリューションを活用できるため、コードの効率性、読みやすさ、保守性が向上します。  

    ![インポートされたパッケージのスクリーンショット。](./media/0126-voice-live-azure-imports.png)
 
1. Web ライブ音声アシスタントは、オーディオ プロセッサと音声アシスタントの 2 つの主要な機能で構成されています。  行 `63-238` で、`AudioProcessor` クラスのコードを確認すると、リアルタイムのオーディオ キャプチャと再生がどのように処理されるかがわかります。 

    ![オーディオ プロセッサ クラスのスクリーンショット。](./media/0126-voice-live-audio-processor.png)

1. `BasicVoiceAssistant` クラスは、行 `240` から始まります。 このクラスのコードでは、VoiceLive Python SDK を使用して VoiceLive 接続からのイベントを処理します。 `BasicVoiceAssistant` が `AudioProcessor` クラス (行`258`など) にどのように依存しているかに注意してください。   

    ![音声アシスタント クラスのスクリーンショット。](./media/0126-voice-live-basic-voice-assistant.png)

1. プレイグラウンド設定と資格情報の構成 (AI の音声、モデル、命令など) は、行 `417` から始まる `parse_arguments` グローバル関数によって処理されます。

    ![parse arguments 関数のスクリーンショット。](./media/0126-voice-live-parse-arguments.png)

1. VoiceLive 資格情報を？するには、コード画面の上部にある **{X}.env 変数** をクリックします。

    ![env 変数のスクリーンショット。](./media/0126-voice-live-env.png)

1. すべてをまとめると、行 `472` から始まる `main` 関数で実行される内容を理解できます。 
    - Azure 資格情報が検証されます ("parse_arguments() が変数 `args` にどのように保存されるかに注意してください")**
    - クライアントが作成されます
    - 音声アシスタントが作成されます ("行 `497` で `BasicVoiceAssistant` を呼び出してアシスタントが作成される方法に注意してください")**
    - 音声アシスタントには、適切なシャットダウンのためのコードが与えられます
    - 音声アシスタントが起動されます 

    ![main 関数のスクリーンショット](./media/0126-voice-live-main-function.png)

## 試してみる 

これでコードの確認が終わりました。次の手順では、コードを自分で試して実装します。 

1. 職場または学校アカウントを持っているユーザーは、コード ウィンドウの上部にある **[Web 用の VS Code で開く]** をクリックして、手順に従うことができます。  
    
    ![コード ウィンドウの上部にある [開く] ボタンのスクリーンショット。](./media/open-in-vs-code-location.png)

    ![Web 用の VS Code 開始ページのスクリーンショット。](./media/0126-vscode-for-web.png)

1. Web バージョンにアクセスできず、ローカル コンピューター上の VS Code にアクセスできる場合は、[voice-live](https://github.com/MicrosoftLearning/mslearn-ai-fundamentals/tree/main/data/voice-live) フォルダーからファイルをダウンロードして、ローカルでコードを試すことができます。 VS Code でファイルを開きます。 フォルダーには、ローカルの VS Code 環境でコードを試すために必要な手順と `codeSample.py` および `example.env` ファイルが含まれています。 

    >**ヒント**: 前の手順で確認した *.env* 変数をコピーして、`example.env` ファイルに貼り付けます。 

    >**ヒント**: 環境を設定するにはターミナルが必要です。 ターミナルは、パッケージのインストール、スクリプトの実行、Git の使用など、システムとプロジェクトを操作するコマンドを実行するためのテキスト ベースのインターフェイスです。 Visual Studio Code で、上部のメニューから [ターミナル] → [新しいターミナル] を選択し、*Bash* シェルを使用します。**
    
## クリーンアップ

これ以上の演習を行わない場合は、不要になったリソースを削除します。 これにより、不要なコストが発生することを防ぎます。

1. [https://portal.azure.com](https://portal.azure.com) で **Azure portal** を開き、作成したリソースを含むリソース グループを選択します。
1. **[リソース グループの削除]** を選び、**リソース グループの名前を入力**して、確定します。 これでリソース グループが削除されます。